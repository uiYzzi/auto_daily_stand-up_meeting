use async_openai::{
    types::{
        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs,
        ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs,
    },
    Client, config::OpenAIConfig,
};
use anyhow::{Result, anyhow};
use std::env;
use futures::StreamExt;
use std::io::{self, Write};

/// AI å®¢æˆ·ç«¯ï¼Œç”¨äºä¸ OpenAI å…¼å®¹çš„ API é€šä¿¡
pub struct AIClient {
    client: Client<OpenAIConfig>,
}

impl AIClient {
    /// åˆ›å»ºæ–°çš„ AI å®¢æˆ·ç«¯
    pub fn new() -> Result<Self> {
        // ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        let api_key = env::var("OPENAI_API_KEY")
            .or_else(|_| env::var("AI_API_KEY"))
            .map_err(|_| anyhow!("æœªè®¾ç½® OPENAI_API_KEY æˆ– AI_API_KEY ç¯å¢ƒå˜é‡"))?;

        let base_url = env::var("OPENAI_BASE_URL")
            .or_else(|_| env::var("AI_BASE_URL"))
            .unwrap_or_else(|_| "https://api.openai.com/v1".to_string());

        println!("AI é…ç½®ä¿¡æ¯:");
        println!("- Base URL: {}", base_url);
        println!("- API Key: {}...{}", 
            &api_key.chars().take(8).collect::<String>(),
            &api_key.chars().rev().take(4).collect::<String>().chars().rev().collect::<String>()
        );

        let config = OpenAIConfig::new()
            .with_api_key(api_key)
            .with_api_base(base_url);

        let client = Client::with_config(config);

        Ok(Self { client })
    }

    /// ç”Ÿæˆæ¯æ—¥ç«™ä¼šæŠ¥å‘Šï¼ˆæµå¼è¾“å‡ºï¼‰
    pub async fn generate_standup_report_stream(&self, prompt: &str) -> Result<()> {
        let system_message = ChatCompletionRequestSystemMessageArgs::default()
            .content("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®ç®¡ç†åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”Ÿæˆæ¯æ—¥ç«™ä¼šæŠ¥å‘Šã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æä¾›çš„æ ¼å¼è¦æ±‚ï¼ŒåŸºäº GitHub PR æ•°æ®ç”Ÿæˆç®€æ´ã€ä¸“ä¸šçš„ç«™ä¼šå†…å®¹ï¼Œå¹¶ä¸”ä¸è¦è¾“å‡ºå¤šä½™çš„å†…å®¹ã€‚")
            .build()?;

        let user_message = ChatCompletionRequestUserMessageArgs::default()
            .content(prompt)
            .build()?;

        let messages = vec![
            ChatCompletionRequestMessage::System(system_message),
            ChatCompletionRequestMessage::User(user_message),
        ];

        let model = env::var("OPENAI_MODEL")
            .or_else(|_| env::var("AI_MODEL"))
            .unwrap_or_else(|_| "gpt-3.5-turbo".to_string());

        let request = CreateChatCompletionRequestArgs::default()
            .model(&model)
            .messages(messages)
            .max_tokens(1000u16)
            .temperature(0.3)
            .stream(true)  // å¯ç”¨æµå¼è¾“å‡º
            .build()?;

        println!("æ­£åœ¨è°ƒç”¨ AI ç”Ÿæˆç«™ä¼šæŠ¥å‘Š...");
        println!("ä½¿ç”¨æ¨¡å‹: {}", model);
        println!("ğŸ¤– AI ç”Ÿæˆçš„æ¯æ—¥ç«™ä¼šæŠ¥å‘Šï¼š");
        println!("======================================");

        // åˆ›å»ºæµå¼è¯·æ±‚
        let mut stream = self.client.chat().create_stream(request).await?;

        // å¤„ç†æµå¼å“åº”
        while let Some(result) = stream.next().await {
            match result {
                Ok(response) => {
                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta.content {
                            print!("{}", delta);
                            io::stdout().flush().unwrap(); // ç¡®ä¿ç«‹å³è¾“å‡º
                        }
                    }
                }
                Err(e) => {
                    eprintln!("\nâŒ æµå¼å“åº”é”™è¯¯: {}", e);
                    return Err(anyhow!("æµå¼å“åº”å¤„ç†å¤±è´¥: {}", e));
                }
            }
        }

        println!();
        println!("======================================");
        println!();
        println!("ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥ç›´æ¥å¤åˆ¶ä¸Šè¿°å†…å®¹åˆ°é£ä¹¦æ±‡æŠ¥åŠŸèƒ½ä¸­");

        Ok(())
    }
}

impl Default for AIClient {
    fn default() -> Self {
        Self::new().expect("æ— æ³•åˆ›å»º AI å®¢æˆ·ç«¯")
    }
}